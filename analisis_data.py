# -*- coding: utf-8 -*-
"""Analisis Data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JTlg3I2Qnzv7hYR3zX_TwDOO8ZesXr7B
"""

!pip install tweet-preprocessor
!pip install textblob
!pip install wordcloud
!pip install nltk

import nltk
#untuk memanggil libaray nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
#melakukan sentiment vader atau sentiment analsisi intensitas
nltk.download("vader_lexicon")
#ini untuk mendowload vader lecivon
import pandas as pd
data = pd.read_csv('comments1.csv', encoding='ISO-8859-1')
#ini nama data yang diupload
data = data.dropna()
print(data.head(10))
#mengeprint data yang telah di upload

sentiments = SentimentIntensityAnalyzer()
data["Positive"] = [sentiments.polarity_scores(i)["pos"] for i in data ["comment_trans"]]
data["Negative"] = [sentiments.polarity_scores(i)["neg"] for i in data ["comment_trans"]]
data["Neutral"] = [sentiments.polarity_scores(i)["neu"] for i in data ["comment_trans"]]
data['Compound'] = [sentiments.polarity_scores(i)["compound"] for i in data ["comment_trans"]]
data.head()

import preprocessor as p
from textblob import TextBlob
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

data.isnull().sum()

import string
string.punctuation
#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
data['comment_trans']= data['comment_trans'].apply(lambda x:remove_punctuation(x))
data.head()

def preprocessing_data(x):
    return p.clean(x)

def tokenize_data(x):
    return p.tokenize(x)

data['comment_clean'] = data['comment_trans'].apply(preprocessing_data)
data['comment_clean']= data['comment_clean'].apply(lambda x: x.lower())
data['comment_clean'] = data['comment_clean'].apply(tokenize_data)
data = data.drop_duplicates()
data.head(10)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

ps = PorterStemmer()

def stemming_data(x):
    return ps.stem(x)

data['comment_clean'] = data['comment_clean'].apply(stemming_data)

data_yt = list(data['comment_clean'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, youtube in enumerate(data_yt):
    analysis = TextBlob(youtube)
    polaritas += analysis.polarity

    if analysis.sentiment.polarity > 0.0:
        total_positif += 1
        status.append('Positif')
    elif analysis.sentiment.polarity == 0.0:
        total_netral += 1
        status.append('Netral')
    else:
        total_negatif += 1
        status.append('Negatif')

    total += 1

print(f'Hasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'\nTotal Data : {total}')

status = pd.DataFrame({'klasifikasi': status})
data['klasifikasi'] = status
data.tail()

from wordcloud import WordCloud, STOPWORDS

def plot_cloud(wordcloud):
    plt.figure(figsize=(12, 8))
    plt.imshow(wordcloud)
    plt.axis("off");

all_words = ' '.join([tweets for tweets in data['comment_clean']])
wordcloud = WordCloud(width = 3000, height = 2000, random_state=3, background_color='white', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(all_words)
plot_cloud(wordcloud)

def show_pie(label, data, legend_title) :
    fig, ax = plt.subplots(figsize=(8, 10), subplot_kw=dict(aspect='equal'))

    labels = [x.split()[-1] for x in label]

    def func(pct, allvals):
        absolute = int(pct/100.*np.sum(allvals))
        return "{:.1f}% ({:d})".format(pct, absolute)

    wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),
                                      textprops=dict(color="w"))

    ax.legend(wedges, labels,
              title= legend_title,
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))

    plt.setp(autotexts, size=10, weight="bold")
    plt.show()

label = ['Positif', 'Negatif', 'Netral']
count_data = [total_positif+1, total_negatif+1, total_netral]

show_pie(label, count_data, "Status")

nltk.download('punkt')

data.head(25)

dataset = data.drop(['Published',	'User',	'Comment',	'comment_trans',	'Positive',	'Negative',	'Neutral',	'Compound'	], axis=1, inplace=False)
dataset = [tuple(x) for x in dataset.to_records(index=False)]

import random

set_positif = []
set_negatif = []
set_netral = []

for n in dataset:
    if(n[1] == 'Positif'):
      set_positif.append(n)
    elif(n[1] == 'Negatif'):
      set_negatif.append(n)
    else:
      set_netral.append(n)

set_positif = random.sample(set_positif, k=int(len(set_positif)/2))
set_negatif = random.sample(set_negatif, k=int(len(set_negatif)/2))
set_netral = random.sample(set_netral, k=int(len(set_netral)/2))

train = set_positif + set_negatif + set_netral

train_set = []

for n in train:
     train_set.append(n)

from textblob.classifiers import NaiveBayesClassifier
cl = NaiveBayesClassifier(train_set)
print('Akurasi Test:', cl.accuracy(dataset))

data_yt = list(data['comment_clean'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, yt in enumerate(data_yt):
    analysis = TextBlob(yt, classifier=cl)

    if analysis.classify() == 'Positif':
        total_positif += 1
    elif analysis.classify() == 'Netral':
        total_netral += 1
    else:
        total_negatif += 1

    status.append(analysis.classify())
    total += 1

print(f'\nHasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'\nTotal Data : {total}')

status = pd.DataFrame({'klasifikasi_bayes': status})
data['klasifikasi_bayes'] = status

label = ['Positif', 'Negatif', 'Netral']
count_data = [total_positif+1, total_negatif+1, total_netral]

show_pie(label, count_data, "Status")

data_eval = [tuple(x) for x in data.to_records(index=False)]

for n in data_eval:
    if n[9] != n[10]:
       print(f'Text: {n[3]}\nClassifier: {n[9]}\nClassifier Bayes: {n[10]} \n')